{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Jupyter Notebook\n",
    "To be able to change code and see changes in the notebook, we need to set the notebook to reload modules.\n",
    "Also, since we use the code in src, we need to add the src folder to the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T15:06:13.634311Z",
     "iopub.status.busy": "2024-05-10T15:06:13.634311Z",
     "iopub.status.idle": "2024-05-10T15:06:13.668808Z",
     "shell.execute_reply": "2024-05-10T15:06:13.668808Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set jupyter to reload modules automatically so we can modify the code and see the changes without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the parent directory to the path so we can import the modules\n",
    "parent_directory = os.path.abspath('..')\n",
    "sys.path.append(parent_directory)\n",
    "sys.path.append('src')\n",
    "sys.path.append(parent_directory + '/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the binvox file\n",
    "name = \"name_of_the_shape_to_bound\"\n",
    "type = \"tet_or_voxel\" # tet or voxel\n",
    "\n",
    "# definition volume. Set to None to use the axis aligned bounding box of the object\n",
    "lower_point = [-1., -1., -1.]\n",
    "upper_point = [ 1.,  1.,  1.]\n",
    "\n",
    "# use pretraining or not\n",
    "use_pretraining = False\n",
    "\n",
    "# scale factor for scaling the object to a different size whitin the definition volume\n",
    "scale_factor = 1.0\n",
    "\n",
    "# dataloader settings\n",
    "number_of_train_test_points = 4_000_000\n",
    "if type == \"voxel\":\n",
    "    num_initial_splits_for_voxel = 4\n",
    "\n",
    "# batch size during data generation to avoid memory overflow\n",
    "batch_size_spec = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type == \"voxel\":\n",
    "    buffer = False\n",
    "elif type == \"tet\":\n",
    "    buffer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure all Training- and Testdata is existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.point_dataloader import PointDataloader\n",
    "from src.tet_mesh import TetrahedralMesh\n",
    "\n",
    "# For tets the points are buffered\n",
    "if type == \"tet\":\n",
    "    mesh = TetrahedralMesh(vtk_file=\"../tet_meshes/\" + name + \".vtk\")\n",
    "\n",
    "    p1 = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=True, type=\"train\", lower_point=lower_point, upper_point=upper_point, buffer=buffer, scale_factor=scale_factor)\n",
    "    p2 = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=True, type=\"test\",  lower_point=lower_point, upper_point=upper_point, buffer=buffer, scale_factor=scale_factor)\n",
    "    del p1, p2\n",
    "\n",
    "    if use_pretraining:\n",
    "        p3 = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=False, type=\"train\", lower_point=lower_point, upper_point=upper_point, buffer=buffer, scale_factor=scale_factor)\n",
    "        p4 = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=False, type=\"test\",  lower_point=lower_point, upper_point=upper_point, buffer=buffer, scale_factor=scale_factor)\n",
    "        del p3, p4\n",
    "\n",
    "    del mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T15:06:13.668808Z",
     "iopub.status.busy": "2024-05-10T15:06:13.668808Z",
     "iopub.status.idle": "2024-05-10T15:06:13.686115Z",
     "shell.execute_reply": "2024-05-10T15:06:13.686115Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time_all = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T15:06:13.703085Z",
     "iopub.status.busy": "2024-05-10T15:06:13.703085Z",
     "iopub.status.idle": "2024-05-10T15:06:14.927948Z",
     "shell.execute_reply": "2024-05-10T15:06:14.927948Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.tet_mesh import TetrahedralMesh\n",
    "from src.cube_mesh import CubeMesh\n",
    "\n",
    "if type == \"tet\":\n",
    "    mesh = TetrahedralMesh(vtk_file=\"../tet_meshes/\" + name + \".vtk\", scale_factor=scale_factor)\n",
    "    print(\"Number of Tetrahedra\", len(mesh.tetrahedra))\n",
    "elif type == \"voxel\":\n",
    "    mesh = CubeMesh(binvox_file=\"../binvox/\" + name + \".binvox\")\n",
    "    print(\"Number of boxes\", len(mesh.upper_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the defined shape\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from src.visualization import create_plt_axes\n",
    "\n",
    "# ax = create_plt_axes(upper_point=upper_point, lower_point=lower_point)\n",
    "# if type == \"tet\":\n",
    "#     mesh.render_shape(ax)\n",
    "# elif type == \"voxel\":\n",
    "#     mesh.render(ax)\n",
    "#     pass\n",
    "\n",
    "# mesh.render_bounding_box(ax)\n",
    "# # set view direction to neg y  \n",
    "# ax.view_init(elev=90, azim=0)\n",
    "# #plt.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Randomly Initialized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "layer_width=50\n",
    "num_hidden_layers=2\n",
    "random_initialized_model_path = name + \"_random_initialized_model.npz\"\n",
    "\n",
    "from src.model import Model\n",
    "\n",
    "# Depending on whether the AABB of the Mesh or a predifined volume is used, the lower and upper point of the model is set to the AABB of the mesh or the predifined volume\n",
    "lower_point_model = lower_point\n",
    "upper_point_model = upper_point\n",
    "if lower_point_model is None and upper_point_model is None:\n",
    "    lower_point_model = mesh.lower_point\n",
    "    upper_point_model = mesh.upper_point\n",
    "\n",
    "model = Model(lower_point_model, upper_point_model, layer_width=layer_width, num_hidden_layers=num_hidden_layers) # Do not inflate (by giving no affine dataloader)\n",
    "\n",
    "print(\"Lower point definition volume model:\", model.lower_point)\n",
    "print(\"Upper point definition volume model:\", model.upper_point)\n",
    "\n",
    "model.save_as_npz_file(random_initialized_model_path)\n",
    "\n",
    "print(\"Model size:\", model.get_number_of_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show initial state of random initialized model\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from src.visualization import create_plt_axes\n",
    "\n",
    "# ax = create_plt_axes()\n",
    "# model.render_with_points(ax, 1_000_000)\n",
    "# ax.set_title(\"random initialized model\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Point Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_dataloader_train_outside = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=True, type=\"train\", lower_point=lower_point, upper_point=upper_point, buffer=buffer)\n",
    "point_dataloader_test_outside  = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=True, type=\"test\",  lower_point=lower_point, upper_point=upper_point, buffer=buffer)\n",
    "\n",
    "if use_pretraining:\n",
    "    point_dataloader_train_inside  = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=False, type=\"train\", lower_point=lower_point, upper_point=upper_point, buffer=buffer)\n",
    "    point_dataloader_test_inside   = PointDataloader(mesh=mesh, target_number_of_points=number_of_train_test_points, batch_size_spec=batch_size_spec, outside=False, type=\"test\",  lower_point=lower_point, upper_point=upper_point, buffer=buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Cut from Data Loader\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from src.visualization import create_plt_axes, Limits\n",
    "\n",
    "# if use_pretraining:\n",
    "#     limits_y = Limits(x_limits=[-1, 1], y_limits=[-0.05, 0.05], z_limits=[-1, 1])\n",
    "#     limits_x = Limits(x_limits=[-0.05, 0.05], y_limits=[-1, 1], z_limits=[-1, 1])\n",
    "\n",
    "#     dataloaders =[point_dataloader_train_inside, point_dataloader_train_outside, point_dataloader_test_inside, point_dataloader_test_outside]\n",
    "#     headlines = [\"point_dataloader_train_inside\", \"point_dataloader_train_outside\", \"point_dataloader_test_inside\", \"point_dataloader_test_outside\"]\n",
    "#     for loader, headline in zip (dataloaders, headlines):\n",
    "#         ax = create_plt_axes()\n",
    "#         loader.render(ax, limits=limits_y)\n",
    "#         ax.view_init(elev=0, azim=-90)\n",
    "#         ax.set_title(headline + \", y-axis\")\n",
    "#         plt.show()\n",
    "\n",
    "#         ax = create_plt_axes()\n",
    "#         loader.render(ax, limits=limits_x)\n",
    "#         ax.view_init(elev=0, azim=0)\n",
    "#         ax.set_title(headline + \", x-axis\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size_pretraining = 32_768\n",
    "\n",
    "start_lr = 0.1\n",
    "lr_factor = 0.5\n",
    "num_epoch_factor = 1.2\n",
    "start_num_epochs = 5\n",
    "lr_reduction_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T15:06:16.839866Z",
     "iopub.status.busy": "2024-05-10T15:06:16.839866Z",
     "iopub.status.idle": "2024-05-10T15:07:21.205622Z",
     "shell.execute_reply": "2024-05-10T15:07:21.205622Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_pretraining:\n",
    "    # addapt dataloader settings for pretraining\n",
    "    dataloaders = [point_dataloader_train_outside, point_dataloader_train_inside]\n",
    "    for i, dataloader in enumerate(dataloaders):\n",
    "        dataloader.batch_size = batch_size_pretraining\n",
    "        print(\"batch_size dataloader \", i, \":\", dataloader.batch_size)\n",
    "\n",
    "    lrs = []\n",
    "    num_epochs = []\n",
    "    current_lr = start_lr\n",
    "    for i in range(lr_reduction_depth):\n",
    "        lrs.append(current_lr * (lr_factor ** i))\n",
    "        num_epochs.append(int(start_num_epochs * (num_epoch_factor ** i)))\n",
    "    print(\"Learn Rates:\", lrs)\n",
    "    print(\"Epoch Numbers:\", num_epochs)\n",
    "    print(\"Total Epochs:\", sum(num_epochs))\n",
    "\n",
    "    from src.trainer import PointTrainer\n",
    "    best_model_path_pretraining = name + \"_pretrained.npz\"\n",
    "\n",
    "    trainer = PointTrainer(point_dataloader_outside_train=point_dataloader_train_outside,\n",
    "                           point_dataloader_outside_test=point_dataloader_test_outside,\n",
    "                           point_dataloader_inside_train=point_dataloader_train_inside,\n",
    "                           point_dataloader_inside_test=point_dataloader_test_inside,\n",
    "                           mesh=mesh)\n",
    "\n",
    "    model = trainer.train(model=model,\n",
    "                          best_model_path=best_model_path_pretraining,\n",
    "                          lrs=lrs,\n",
    "                          num_epochs=num_epochs)\n",
    "    \n",
    "    model = Model(lower_point_model, upper_point_model, path=best_model_path_pretraining)\n",
    "\n",
    "    trainer.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Affine Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.affine_dataloader import AffineDataloader\n",
    "\n",
    "batch_size_affine_dataloader = 2048\n",
    "\n",
    "if type == \"tet\":\n",
    "    affine_dataloader = AffineDataloader(mesh=mesh, batch_size=batch_size_affine_dataloader)\n",
    "elif type == \"voxel\":\n",
    "    affine_dataloader = AffineDataloader(mesh=mesh, batch_size=batch_size_affine_dataloader, num_initial_splits_for_voxel=num_initial_splits_for_voxel)\n",
    "print(\"Number of samples in affine dataloader: \", affine_dataloader.get_num_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show general boxes\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from src.visualization import create_plt_axes\n",
    "\n",
    "# ax = create_plt_axes()\n",
    "# affine_dataloader.render(ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import Model\n",
    "\n",
    "if use_pretraining:\n",
    "    model = Model(lower_point_model, upper_point_model, path=best_model_path_pretraining, affine_dataloader=affine_dataloader)\n",
    "else:\n",
    "    model = Model(lower_point_model, upper_point_model, path=random_initialized_model_path, affine_dataloader=affine_dataloader)\n",
    "\n",
    "inflated_model_path = name + \"_inflated.npz\"\n",
    "model.save_as_npz_file(inflated_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T15:07:21.205622Z",
     "iopub.status.busy": "2024-05-10T15:07:21.205622Z",
     "iopub.status.idle": "2024-05-10T15:07:21.266060Z",
     "shell.execute_reply": "2024-05-10T15:07:21.266060Z"
    }
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "# auto determine lambda and lr settings\n",
    "lambda_start  = 1\n",
    "lambda_factor = 1.3\n",
    "start_lr = 0.001\n",
    "lr_reduction_factor = 0.7\n",
    "max_tries = 50\n",
    "\n",
    "# training settings\n",
    "lr_factor = 0.7\n",
    "num_epoch_factor = 1.0\n",
    "epochs_per_lr = 500\n",
    "lr_reduction_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the batch size to the number of samples in the affine dataloader\n",
    "num_batches_affine_dataloader = len(affine_dataloader)\n",
    "point_dataloader_train_outside.change_num_batches(num_batches_affine_dataloader)\n",
    "\n",
    "print(\"Number of batches affine dataloader:\", len(affine_dataloader))\n",
    "print(\"New number of batches point dataloader:\", len(point_dataloader_train_outside))\n",
    "\n",
    "print(\"Batch size affine dataloader:\", affine_dataloader.batch_size)\n",
    "print(\"New batch size for point dataloader:\", point_dataloader_train_outside.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T15:07:21.266060Z",
     "iopub.status.busy": "2024-05-10T15:07:21.266060Z",
     "iopub.status.idle": "2024-05-10T18:40:18.951473Z",
     "shell.execute_reply": "2024-05-10T18:40:18.951473Z"
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "from src.model import Model\n",
    "from src.trainer import OptimizationTrainer\n",
    "from src.evaluation import test_with_dataloader_points\n",
    "\n",
    "final_best_model_path_optimization = name + \"_optimized.npz\"\n",
    "intermediate_best_model_path = name + \"_best_model.npz\"\n",
    "\n",
    "# Initialize intermediate best and final best model with the inflated model\n",
    "model = Model(lower_point_model, upper_point_model, path=inflated_model_path)\n",
    "\n",
    "model.save_as_npz_file(intermediate_best_model_path)\n",
    "model.save_as_npz_file(final_best_model_path_optimization)\n",
    "\n",
    "print(\"Number of parameters in model: \", model.get_number_of_parameters())\n",
    "\n",
    "# Initial best accuracy with inflated model\n",
    "best_found_acc, _ = test_with_dataloader_points(model, point_dataloader_test_outside)\n",
    "\n",
    "trainer = OptimizationTrainer(box_dataloader=affine_dataloader, train_dataloader_outside=point_dataloader_train_outside, test_dataloader_outside=point_dataloader_test_outside, mesh=mesh)\n",
    "\n",
    "cnt_failed_runs = 0\n",
    "current_lr = start_lr\n",
    "success = False\n",
    "# while there was no success, try again.\n",
    "while not success and cnt_failed_runs < max_tries:\n",
    "    # create lr and num_epochs lists for the current run\n",
    "    lrs = []\n",
    "    num_epochs = []\n",
    "    for i in range(lr_reduction_depth):\n",
    "        lrs.append(current_lr * (lr_factor ** i))\n",
    "        num_epochs.append(int(epochs_per_lr * (num_epoch_factor ** i)))\n",
    "\n",
    "    print(\"lrs: \", lrs)\n",
    "    print(\"num_epochs: \", num_epochs)\n",
    "\n",
    "    # Load inflated model\n",
    "    model = Model(lower_point_model, upper_point_model, path=inflated_model_path)\n",
    "\n",
    "    # numerical errors can cause the learning to crash (e.g. by producing NaN weights). Except and try again\n",
    "    try:\n",
    "        # training\n",
    "        trainer.train(model=model, best_model_path=intermediate_best_model_path, lrs=lrs, num_epochs=num_epochs, lambda_start=lambda_start, lambda_increase_factor=lambda_factor, max_lambda_increases=max_tries)\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        cnt_failed_runs += 1\n",
    "    \n",
    "    model = Model(lower_point_model, upper_point_model, path=intermediate_best_model_path)\n",
    "    accuracy, _ = test_with_dataloader_points(model, point_dataloader_test_outside)\n",
    "\n",
    "    print(\"Accuracy from last training:\", accuracy, \"Best found Accuracy: \", accuracy)\n",
    "    \n",
    "    if accuracy > best_found_acc:\n",
    "        best_found_acc = accuracy\n",
    "        model.save_as_npz_file(final_best_model_path_optimization)\n",
    "        print(\"Saved model as: \", final_best_model_path_optimization)\n",
    "    \n",
    "    current_lr = current_lr * lr_reduction_factor\n",
    "\n",
    "os.remove(intermediate_best_model_path)\n",
    "\n",
    "model = Model(lower_point_model, upper_point_model, path=final_best_model_path_optimization)\n",
    "\n",
    "trainer.plot_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda reduction settings\n",
    "lambda_reduction_depth = 10\n",
    "lambda_reduction_epochs = 50\n",
    "lambda_reduction_factor = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:40:19.600106Z",
     "iopub.status.busy": "2024-05-10T18:40:19.600106Z",
     "iopub.status.idle": "2024-05-10T19:18:48.281394Z",
     "shell.execute_reply": "2024-05-10T19:18:48.281394Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from src.trainer import LambdaReductionTrainer\n",
    "\n",
    "start_time_lambda_reduction = time.time()\n",
    "\n",
    "intermediate_best_model_path = name + \"_best_model.npz\"\n",
    "final_best_model_path_lambda_reduction = name + \"_lambda_reduced.npz\"\n",
    "\n",
    "# Load the last lambda and lr values from the optimization trainer\n",
    "current_lambda=trainer.current_lambda\n",
    "print(\"last lambda: \", current_lambda)\n",
    "last_lr = lrs[-1]\n",
    "print(\"last lr: \", last_lr)\n",
    "\n",
    "# create lr and num_epochs lists for the current run\n",
    "lrs = []\n",
    "num_epochs = []\n",
    "lrs.append(last_lr)\n",
    "num_epochs.append(int(lambda_reduction_epochs/2))\n",
    "lrs.append(last_lr * 0.5)\n",
    "num_epochs.append(int(lambda_reduction_epochs/2))\n",
    "\n",
    "# create the trainer\n",
    "trainer = LambdaReductionTrainer(box_dataloader=affine_dataloader,\n",
    "                                train_dataloader_outside=point_dataloader_train_outside,\n",
    "                                test_dataloader_outside=point_dataloader_test_outside,\n",
    "                                mesh=mesh)\n",
    "\n",
    "# Load the best found model from the optimization\n",
    "model = Model(lower_point_model, upper_point_model, path=final_best_model_path_optimization)\n",
    "model.save_as_npz_file(final_best_model_path_lambda_reduction)\n",
    "\n",
    "found_new_best = False\n",
    "\n",
    "# inizialize the best found accuracy with the accuracy of the model before the lambda reduction\n",
    "best_found_acc, _ = test_with_dataloader_points(model, point_dataloader_test_outside)\n",
    "\n",
    "for i in range(lambda_reduction_depth):\n",
    "    current_lambda *= lambda_reduction_factor\n",
    "    print(\"Round\", i+1, \"of\", lambda_reduction_depth, \"new lambda:\", current_lambda)\n",
    "\n",
    "    # In case the model gets numerically instable, catch the exception and continue with the next lambda value\n",
    "    try:\n",
    "        model = trainer.train(model=model,\n",
    "                              best_model_path=intermediate_best_model_path,\n",
    "                              lrs=lrs,\n",
    "                              num_epochs=num_epochs, \n",
    "                              lambda_start=current_lambda)\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Training stopped due to error. Continue with next lambda value.\")\n",
    "    \n",
    "    model = Model(lower_point_model, upper_point_model, path=intermediate_best_model_path)\n",
    "\n",
    "    accuracy, _ = test_with_dataloader_points(model, point_dataloader_test_outside)\n",
    "    print(\"Best found Accuracy: \", accuracy)\n",
    "    \n",
    "    if accuracy > best_found_acc:\n",
    "        best_found_acc = accuracy\n",
    "        model.save_as_npz_file(final_best_model_path_lambda_reduction)\n",
    "        found_new_best = True\n",
    "        print(\"Saved model as: \", final_best_model_path_lambda_reduction)\n",
    "\n",
    "model = Model(lower_point_model, upper_point_model, path=final_best_model_path_lambda_reduction)\n",
    "\n",
    "os.remove(intermediate_best_model_path)\n",
    "    \n",
    "print(\"Time for lambda reduction: \", (time.time() - start_time_lambda_reduction) / 60, \"min, found new best: \", found_new_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique model name for identification\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "save_model_path = name + \"_\" + dt_string + \".npz\"\n",
    "print(save_model_path)\n",
    "\n",
    "# Load best model\n",
    "model = Model(lower_point_model, upper_point_model, path=final_best_model_path_lambda_reduction)\n",
    "\n",
    "# save model in npz format (giving additional information about the architecture)\n",
    "model.save_as_npz_file(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Time for full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = time.time() - start_time_all\n",
    "print(\"Total time: \", total_time, \"s\")\n",
    "print(\"Total time: \", total_time / 60, \"min\")\n",
    "\n",
    "with open(\"times.txt\", \"a\") as file:\n",
    "    file.write(name + \" \" + str(total_time) + \" s \" + save_model_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Validation Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_val_points = 5_000_000\n",
    "\n",
    "from src.validation_points import ValidationPoints\n",
    "validation_points = ValidationPoints(mesh=mesh, number_of_points=number_of_val_points, batch_size_spec=batch_size_spec, buffer=buffer, lower_point=lower_point, upper_point=upper_point, scale_factor=scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Cut from Validation Points\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from src.visualization import create_plt_axes, Limits\n",
    "\n",
    "# limits_y = Limits(x_limits=[-1, 1], y_limits=[-0.05, 0.05], z_limits=[-1, 1])\n",
    "# limits_x = Limits(x_limits=[-0.05, 0.05], y_limits=[-1, 1], z_limits=[-1, 1])\n",
    "\n",
    "# ax = create_plt_axes()\n",
    "# validation_points.render(ax, limits=limits_y)\n",
    "# ax.view_init(elev=0, azim=-90)\n",
    "# ax.set_title(\"validation_points, y-axis\")\n",
    "# plt.show()\n",
    "\n",
    "# ax = create_plt_axes()\n",
    "# validation_points.render(ax, limits=limits_x)\n",
    "# ax.view_init(elev=0, azim=0)\n",
    "# ax.set_title(\"validation_points, x-axis\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import determine_FPR_FNR, sanity_check_with_validation_points, test_with_parallelepipeds\n",
    "\n",
    "def test_model(path, check_for_false_negatives):\n",
    "    model_to_test = Model(lower_point_model, upper_point_model, path=path)\n",
    "    determine_FPR_FNR(model_to_test, validation_points)\n",
    "\n",
    "    if check_for_false_negatives:\n",
    "        false_negatives, _ =test_with_parallelepipeds(model_to_test, affine_dataloader)\n",
    "        assert false_negatives == 0, \"There are false negatives in the inflated model\"\n",
    "        print(\"No false negatives found.\")\n",
    "\n",
    "        passed_sanity_check = sanity_check_with_validation_points(model_to_test, validation_points)\n",
    "        assert passed_sanity_check, \"Sanity check failed\"\n",
    "        print(\"Sanity check passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random initialized model. This model can have false negatives.\n",
    "#test_model(random_initialized_model_path, check_for_false_negatives=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pretrained model. This model can have false negatives.\n",
    "if use_pretraining:\n",
    "    test_model(best_model_path_pretraining, check_for_false_negatives=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate inflated model. This model must not have false negatives.\n",
    "test_model(inflated_model_path, check_for_false_negatives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized model. This model must not have false negatives.\n",
    "#test_model(final_best_model_path_optimization, check_for_false_negatives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model after lambda reduction.\n",
    "test_model(final_best_model_path_lambda_reduction, check_for_false_negatives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import test_accuracy_with_validation_points\n",
    "\n",
    "model = Model(lower_point_model, upper_point_model, path=final_best_model_path_lambda_reduction)\n",
    "\n",
    "accuracy, points_inside_bounding_volume_div_points_inside_object = test_accuracy_with_validation_points(model, validation_points)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Points inside bounding volume div points inside object: \", points_inside_bounding_volume_div_points_inside_object)\n",
    "\n",
    "#write to result.txt if file exists\n",
    "with open(\"results.txt\", \"a\") as file:\n",
    "    file.write(name + \" \" + str(best_found_acc) + \" \" + str(points_inside_bounding_volume_div_points_inside_object) + \" \" + save_model_path + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Resulting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "view_axis = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.visualization import create_plt_axes, create_plt_axes_2d\n",
    "\n",
    "def show_model(path, title):\n",
    "    model_to_visualize = Model(lower_point_model, upper_point_model, path=path)\n",
    "    \n",
    "    ax = create_plt_axes()\n",
    "    model_to_visualize.render_with_points(ax, num_points=2_000_000, set_view_axis=view_axis)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "    plt_axes = create_plt_axes_2d()\n",
    "    model_to_visualize.show_difference_to_mesh_2d(plt_axes, 1_000_000, axis=\"y\", axis_value=0.0, mesh_to_show_difference_to=mesh, buffer=buffer, scale_factor=scale_factor)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show random initialized model\n",
    "show_model(random_initialized_model_path, \"random initialized model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pretrained model\n",
    "if use_pretraining:\n",
    "    show_model(best_model_path_pretraining, \"pretrained model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show inflated model\n",
    "show_model(inflated_model_path, \"inflated model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show optimized model\n",
    "show_model(final_best_model_path_optimization, \"optimized model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model after lambda reduction\n",
    "show_model(final_best_model_path_lambda_reduction, \"model after lambda reduction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bounding_box",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
